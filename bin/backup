#!/usr/bin/env python

from datetime import date
from itertools import chain, cycle, islice
from optparse import OptionParser
import os
import Queue
import subprocess
from threading import Thread
import yaml

PROGDIR = os.path.abspath(os.path.dirname(__file__))
RBDBACKUP = os.path.join(PROGDIR, 'rbdbackup')
RGWBACKUP = os.path.join(PROGDIR, 'rgwbackup')
RSYNCBACKUP = os.path.join(PROGDIR, 'rsyncbackup')


def roundrobin(*iterables):
    "roundrobin('ABC', 'D', 'EF') --> A D E B F C"
    # Recipe credited to George Sakkis
    # Taken verbatim from the itertools documentation.
    pending = len(iterables)
    nexts = cycle(iter(it).next for it in iterables)
    while pending:
        try:
            for next in nexts:
                yield next()
        except StopIteration:
            pending -= 1
            nexts = cycle(islice(nexts, pending))


class Task(object):
    def run(self):
        print ' '.join(self.command)
        subprocess.check_call(self.command, close_fds=True)


class BucketTask(Task):
    def __init__(self, settings, name, force_s3_acls=False):
        Task.__init__(self)
        out_dir = os.path.join(settings['root'], 'buckets', name)
        self.command = [RGWBACKUP, '-v', settings['rgw-server'], name,
                out_dir]
        if settings.get('rgw-secure', False):
            self.command.append('-s')
        if force_s3_acls:
            self.command.append('-A')


class ImageTask(Task):
    SECTION = 'images'

    def __init__(self, settings, pool, name, friendly_name):
        Task.__init__(self)
        out_path = os.path.join(settings['root'], 'blockdevs', pool,
                self.SECTION, friendly_name)
        self.command = [RBDBACKUP, pool, name, out_path]


class SnapshotTask(ImageTask):
    SECTION = 'snapshots'

    def __init__(self, settings, pool, name, friendly_name):
        ImageTask.__init__(self, settings, pool, name, friendly_name)
        self.command.append('-s')


class MachineTask(Task):
    def __init__(self, settings, hostname, info):
        Task.__init__(self)
        out_path = os.path.join(settings['root'], 'machines',
                hostname.split('.')[0])
        self.command = [RSYNCBACKUP, '-v', hostname, out_path]
        for mount in info['mounts']:
            self.command.extend(['-m', mount])
        for rule in chain(settings.get('rsync-exclude', []),
                info.get('exclude', [])):
            self.command.extend(['-x', rule])
        if 'pre' in info:
            self.command.extend(['--pre', info['pre']])
        if 'post' in info:
            self.command.extend(['--post', info['post']])


class TaskList(object):
    def __init__(self, config, buckets=False, images=False,
            machines=False, force_s3_acls=False):
        machine_tasks = []
        if machines:
            for hostname, info in config.get('machines', {}).items():
                machine_tasks.append(MachineTask(settings, hostname, info))

        bucket_tasks = []
        if buckets:
            for name in config.get('buckets', []):
                bucket_tasks.append(BucketTask(settings, name,
                        force_s3_acls=force_s3_acls))

        image_tasks = []
        if images:
            for pool, info in config.get('images', {}).items():
                for friendly_name, name in info.items():
                    image_tasks.append(ImageTask(settings, pool, name,
                            friendly_name))
            for pool, info in config.get('snapshots', {}).items():
                for friendly_name, name in info.items():
                    image_tasks.append(SnapshotTask(settings, pool, name,
                            friendly_name))

        # Try to interleave access to different types of systems
        self._tasks = list(roundrobin(machine_tasks, bucket_tasks,
                image_tasks))

    def run(self, thread_count=4):
        queue = Queue.Queue()
        for task in self._tasks:
            queue.put(task)

        threads = [Thread(target=self._worker, args=[queue])
                for n in range(thread_count)]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

    def _worker(self, queue):
        while True:
            try:
                task = queue.get_nowait()
            except Queue.Empty:
                return
            task.run()


def create_snapshot(settings):
    today = date.today().strftime('%Y%m%d')
    backup_lv = settings['backup-lv']
    vg = backup_lv.split('/')[0]
    with open('/dev/null', 'r+') as null:
        # Give up eventually in case test keeps failing
        for n in range(1, 100):
            snapshot_lv = '%s-%d' % (today, n)
            ret = subprocess.call(['sudo', 'lvs',
                    '%s/%s' % (vg, snapshot_lv)], stdout=null, stderr=null)
            if ret:
                break
        else:
            raise Exception("Couldn't locate unused snapshot LV")
    subprocess.check_call(['sudo', 'lvcreate', '-s', backup_lv, '-p', 'r',
            '-n', snapshot_lv])


if __name__ == '__main__':
    parser = OptionParser(usage='Usage: %prog [options] <config_path>')
    parser.add_option('-j', '--jobs', dest='thread_count', type=int, default=4,
            help='number of backups to run in parallel [4]')
    parser.add_option('-b', '--buckets', dest='buckets', action='store_true',
            help='dump buckets')
    parser.add_option('-A', '--force-s3-acls', dest='force_s3_acls',
            action='store_true',
            help='update ACLs for unmodified S3 objects')
    parser.add_option('-i', '--images', dest='images', action='store_true',
            help='dump images and snapshots')
    parser.add_option('-m', '--machines', dest='machines', action='store_true',
            help='dump machines')
    parser.add_option('-s', '--snapshot', dest='snapshot', action='store_true',
            help='snapshot backup volume')

    (opts, args) = parser.parse_args()
    try:
        config_path = args[0]
    except IndexError:
        parser.error('Missing argument')

    with open(config_path) as fh:
        config = yaml.safe_load(fh)
    settings = config['settings']

    tasks = TaskList(config, buckets=opts.buckets, images=opts.images,
            machines=opts.machines, force_s3_acls=opts.force_s3_acls)
    tasks.run(opts.thread_count)
    if opts.snapshot:
        create_snapshot(settings)
